---
title: "Introduction to totalvis"
author: "Nick Seedorff and Grant Brown"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Introduction to totalvis}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

`totalvis` is a model agnostic vizualization tool used to summarize complex relationships in black-box models. The package relies on a PCA based transformation of the training data to group correlated features and display their total effect on the response. This vignette demonstrates base usage of 'totalvis' by reproducing all figures used in the associated paper, as well as introducing a few additional components.

The vignette makes use of the following packages:

```{r message=FALSE}
library(MASS)
library(randomForest)
library(MachineShop)
library(gbm)
library(e1071)
library(nnet)
library(caret)
library(kknn)
#devtools::install_github("nickseedorff/totalvis", 
#                       auth = "8d5a20d386acf526450bff89472f1f4b7482444e", 
#                       build_vignettes = T)
#devtools::install_github("nickseedorff/totalvis")
library(totalvis)
```

# Simulated examples

Through three simulated examples we display several characters of `totalvis`. The first example addresses the idea of a 'total effect' interpretation of the principal components defined by the transformation. Following this, examples address the use of the `partial_effects` plot as a diagnostic tool and that provides further insight into the dependence patterns.

## Total effect
This example demonstates the 'total effect' notion of `totalvis` plots. Marginally, the covariates affect the outcome in piece-wise linear fashions. The partial depedence plots (PDPs) correctly identify the marginal impacts, while `totalvis` groups the features based on their correlation and presents a visual summary of their combined effect.

```{r fig.width=6.5, fig.height = 4.5}
## Generate data
set.seed(2021)
X <- mvrnorm(n = 1000, mu = c(1, 1.2), 
                   Sigma = matrix(c(1, 0.2, 0.2, 1), nrow = 2))
colnames(X) <- c("X1", "X2")
X_new <- cbind(X[, 1], as.numeric(X[, 1] > 0.5), 
               as.numeric(X[, 2] < 0.5), X[, 2])

## Covariates have piece-wise linear behavior
y = 2 * X_new[, 1] * (1 - X_new[, 2]) + X_new[, 2] + X_new[, 3] + 
  2 * X_new[, 4] * (1 - X_new[, 3]) + rnorm(1000, 0, 0.2)

par(mfrow=c(1, 2))
plot(X[, 1], y, xlab = "X1")
plot(X[, 2], y, xlab = "X2")
par(mfrow=c(1, 1))

## Train the model
mod_frame <- ModelFrame(X, y)
knn_mod <- fit(mod_frame, model = KNNModel)

## Partial depedence of X1
plot(totalvis(knn_mod, X, feature = "X1", pc_num = NULL))

## Partial depedence of X1
plot(totalvis(knn_mod, X, feature = "X2", pc_num = NULL))

## Total effect plot
plot(totalvis(knn_mod, X), legend_cex = 0.9)

```

## Correlated features with different effects

When using PCA to transform the training data, the assocation with the response is not taken into account. Thus, the principal components should be assessed to see if the covariates behave constructively on the outcome. We make use of the `partial_effects` plot, which serves as a diagnostic tool to check these assumptions and provides interesting insight into the depedence patterns.

```{r fig.width=6.5, fig.height = 4.5}
## Generate data
set.seed(2021)
X <- as.data.frame(mvrnorm(n = 1000, mu = c(2, 2), 
                           Sigma = matrix(c(1, 0.9, 0.9, 1), nrow = 2)))
colnames(X) <- c("X1", "X2")
y = X[, 1] - X[, 2] + rnorm(1000, 0, 0.1) 

## Train model
gbm_mod <- gbm(y ~ ., data = data.frame(y, X), n.trees = 200, 
               distribution = "gaussian")

## Total effect plot
plot(totalvis(gbm_mod, X), legend_loc = "topleft")

## Partial effects plot
plot(partial_effects(gbm_mod, X), legend_cex = 0.88)

```

## Correlated feature with no effect on the outcome

In the final simulated example, we assess `totalvis`' ability to correctly identify inputs which are relevant to the response. To do this, we generate two correlated features, but only one acts on the outcome. To evaulate the diagnostic plot in the context of the `totalvis` curve, we make use of the `differenced = FALSE` argument. 

```{r include = F,  fig.width=6.5, fig.height = 4.5}
## Generate data
set.seed(2021)
X <- MASS::mvrnorm(n = 1000, mu = c(2, 2), 
                   Sigma = matrix(c(1, 0.8, 0.8, 1), nrow = 2))
colnames(X) <- c("X1", "X2")

y = as.factor((X[, 1] + rnorm(1000, 0, 0.1)) > 1.5)  

## Train model, plot pdp_pca
mod <- train(X, y , method = "nnet", trace = F)

## Pin plot
plot(totalvis(mod, X, pin = "X2", type = "classification"))

## Partial effects plot
plot(partial_effects(mod, X, type = "classification"), differenced = FALSE,
     legend_loc = "topleft")
```

# Communities and Crimes Example

## Extrapolation error
For some related methods, namely partial depedence plots (PDPs) and individual expectation (ICE) curves, their produceds generate observations that are far outside of the joint distribution of the traing data. An example of this can be refered below and is a common occurance with highly correlated features. 

```{r fig.width=6.5, fig.height = 4.5}
set.seed(2021)
X <- MASS::mvrnorm(n = 500, mu = c(2, 2), 
                   Sigma = matrix(c(1, 0.9, 0.9, 1), nrow = 2))
colnames(X) <- c("X1", "X2")
plot(X)
points(rep(1.5, 500), X[, 2], col = "red")
```

## Feature importance and correlation

Partial depedence plots are often used to visualize the effects of the most important features in black-box models. The example uses a `randomForest` model from the `randomForest` package, which has readily available measures of feature importance. We select one of the primary features, 'pctKids2Par', for further exploration. 'pctKids2Par' presents highly correlation with several other predictiors, limiting the appropriatness of PDPs and ICE curves.  

```{r  fig.width=6.5, fig.height = 4.5}
## Get data
crimes <- read.csv(paste0("https://archive.ics.uci.edu/ml/machine-learning-",
                          "databases/00211/CommViolPredUnnormalizedData.txt"), 
                   header = F)

## Add column names
crimes_descr <- readLines(paste0("http://archive.ics.uci.edu/ml/datasets/",
                                 "Communities+and+Crime+Unnormalized"))
crimes_descr <- strsplit(crimes_descr[grepl(pattern = "<br>@attribute", 
                                            x = crimes_descr)], split = " ")
colnames(crimes) <- vapply(crimes_descr, FUN = function(rw) {rw[[2]]}, 
                           FUN.VALUE = "STRING")

## Replace "?" with NA
crimes[crimes == "?"] <- NA
crimes <- crimes[, colSums(is.na(crimes)) == 0]

## Convert all columns to numeric, add outcome
col_start <- which(names(crimes) == "pop")
end_start <- which(names(crimes) == "pctOfficDrugUnit")
data <- as.data.frame(sapply(crimes[, col_start:end_start], as.numeric))
outcome <- crimes$murdPerPop

## Train the model
set.seed(2021)
rf_mod <- randomForest(data, outcome, ntree = 5)


## Feature importance
varImpPlot(rf_mod, n.var = 10, main = "Feature Importance", pt.cex = 2)
cor_vals <- abs(cor(data[, "pctKids2Par"], 
                    data[, setdiff(colnames(data), "pctKids2Par")]))

## Correlation
hist(cor_vals, breaks = 30, xlab = "abs(correlation)", 
     main = "Correlation with pctKids2Par")
num <- quantile(cor_vals, 0.8)
abline(v = num, col = "red", lwd = 3)
text(x = num + 0.23, y = 10, " 80th Percentile")
```

## totalvis plots

The primary function `totalvis` is to display PDPs of specified principal components. The top contributors to the linear combination are presented in the legend, where contribution is defined by the magnitudes of the loadings. In cases such as this, there is a clear latent factor captured by the top features, income or economic status. Using this, we can interpret the impact that neighboor income levels has on the response. 

```{r fig.width=6.5, fig.height = 4.5}
## totalvis plot
df <- plot(totalvis(rf_mod, data), num_load = 8, legend_cex = 1.0)
knitr::kable(df, row.names = F, align = "lc")
```

To associated the total effect of a group of predictors to a covariate of interest, we look towards the 'pinplot'. We now plot the total effect against a more interpretable x-axis. Comparing this to a partial dependence plot of the same feature, we see a much larger range of the predicted values.

```{r fig.width=6.5, fig.height = 4.5}
## pinplot
plot(totalvis(rf_mod, data, pin = "pctKids2Par"))

## pdp
plot(totalvis(rf_mod, data, feature = "pctKids2Par", pc_num = NULL))
```

As previously discussed, their are a variety of insight to be taken for the `partial_effects` plot. In these instance, the lines appear to be shifting in unison and the underlying assumptions of `totalvis` appear to be reasonable. The default is the plot a standardized or 'differenced' version of the predictions, but lines can also be added to the original `totalvis` plot.

```{r fig.width=6.5, fig.height = 4.5}
## partial_effects (default)
diag_obj <- partial_effects(rf_mod, data, num_load = 8)
plot(diag_obj, legend_loc = "left", legend_cex = 0.74)

## partial_effects (non-differenced)
plot(diag_obj, differenced = FALSE, legend_loc = "topleft", legend_cex = 0.9)
```

## Additionally functionality

Given the reliance on the package to PCA, we offer a wrapper for the `screeplot` function that works with several of the `totalvis` objects.

```{r fig.width=6.5, fig.height = 4.5}
## Wrapper to scree plot
scree_plot(totalvis(rf_mod, data))
```

As a final addition to the package, we offer simple ICE curves. The default is to center the curves so they all have the same starting value and differences in tracjeories are more obvious. This can be changed with the `center = FALSE` argument
```{r fig.width=6.5, fig.height = 4.5}
## ice plot (default is centered)
vis_obj <- totalvis(rf_mod, data, ice = T, feature = "pctKids2Par")
plot(vis_obj, legend_cex = 1.0)

## ice plot (uncentered)
plot(vis_obj, legend_cex = 1.0, center = FALSE)
```
