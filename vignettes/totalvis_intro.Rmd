---
title: "Introduction to totalvis"
author: "Nick Seedorff and Grant Brown"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Introduction to totalvis}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This vignette demonstrates base usage of the `totalvis` by reproducing all figures used in the associated paper, as well a few additional applications `totalvis` visualizes total effects in black box models by grouping correlated features through a PCA transformation and then evaulates the partial dependence of the principal components. Functionality and use are presented through several simulated examples and a real data application using the Communities and Crime Unnormalized data set from the UCI respository.   

The vignette makes use of the following packages:

```{r message=FALSE}
library(MASS)
library(randomForest)
library(MachineShop)
library(gbm)
library(e1071)
library(nnet)
library(caret)
library(kknn)
#devtools::install_github("nickseedorff/totalvis", 
#                       auth = "8d5a20d386acf526450bff89472f1f4b7482444e", 
#                       build_vignettes = T,
#                       build_opts = c("--no-resave-data", "--no-manual"))
#devtools::install_github("nickseedorff/totalvis")
library(totalvis)
```

# Simulated examples

## Total effect
This example demonstates the 'total effect' notion of `totalvis` plots. Marginally, the covariates affect the outcome in piece-wise linear fashions. The partial depedence plots (PDPs) correctly identify the marginal impacts, while `totalvis` groups the features based on their correlation and presents a visual summary of their combined effect.

```{r fig.width=6.5, fig.height = 4.5}
## Generate data
set.seed(2021)
X <- mvrnorm(n = 1000, mu = c(1, 1.2), 
                   Sigma = matrix(c(1, 0.2, 0.2, 1), nrow = 2))
colnames(X) <- c("X1", "X2")
X_new <- cbind(X[, 1], as.numeric(X[, 1] > 0.5), 
               as.numeric(X[, 2] < 0.5), X[, 2])

## Covariates have piece-wise linear behavior
y = 2 * X_new[, 1] * (1 - X_new[, 2]) + X_new[, 2] + X_new[, 3] + 
  2 * X_new[, 4] * (1 - X_new[, 3]) + rnorm(1000, 0, 0.2)

par(mfrow=c(1, 2))
plot(X[, 1], y, xlab = "X1")
plot(X[, 2], y, xlab = "X2")
par(mfrow=c(1, 1))

## Train the model
mod_frame <- ModelFrame(X, y)
knn_mod <- fit(mod_frame, model = KNNModel)

## Partial depedence of X1
plot(totalvis(knn_mod, X, feature = "X1", pc_num = NULL))

## Partial depedence of X1
plot(totalvis(knn_mod, X, feature = "X2", pc_num = NULL))

## Total effect plot
plot(totalvis(knn_mod, X), legend_cex = 0.9)

```

## Correlated features with different effects

When using PCA to transform the training data, the assocation with the response is not taken into account. Thus, the principal components should be assessed to see if the covariates behave constructively on the outcome. We make use of the `partial_effects` plot, which serves as a diagnostic tool to check these assumptions and provides interesting insight into the depedence patterns.

```{r fig.width=6.5, fig.height = 4.5}
## Generate data
set.seed(2021)
X <- as.data.frame(mvrnorm(n = 1000, mu = c(2, 2), 
                           Sigma = matrix(c(1, 0.9, 0.9, 1), nrow = 2)))
colnames(X) <- c("X1", "X2")
y = X[, 1] - X[, 2] + rnorm(1000, 0, 0.1) 

## Train model
gbm_mod <- gbm(y ~ ., data = data.frame(y, X), n.trees = 200, 
               distribution = "gaussian")

## Total effect plot
plot(totalvis(gbm_mod, X), legend_loc = "topleft")

## Partial effects plot
plot(partial_effects(gbm_mod, X), legend_cex = 0.88)

```

## Correlated feature with no effect on the outcome; classification

In the final simulated example, we assess `totalvis`' ability to correctly identify inputs which are relevant to the response. To do this, we generate two correlated features, but only one acts on the outcome. To evaulate the diagnostic plot in the context of the `totalvis` curve, we make use of the `differenced = FALSE` argument. 

```{r include = F,  fig.width=6.5, fig.height = 4.5}
## Generate data
set.seed(2021)
X <- MASS::mvrnorm(n = 1000, mu = c(2, 2), 
                   Sigma = matrix(c(1, 0.8, 0.8, 1), nrow = 2))
colnames(X) <- c("X1", "X2")

y = as.factor((X[, 1] + rnorm(1000, 0, 0.1)) > 1.5)  

## Train model, plot pdp_pca
mod <- train(X, y , method = "nnet", trace = F)

## Pin plot
plot(totalvis(mod, X, pin = "X2", type = "classification"))

## Partial effects plot
plot(partial_effects(mod, X, type = "classification"), differenced = FALSE,
     legend_loc = "topleft")
```

# Communities and Crimes Example

## Extrapolation error
For some related methods, namely partial depedence plots (PDPs) and individual expectation (ICE) curves, their produceds generate observations that are far outside of the joint distribution of the traing data. An example of this can be refered below and is a common occurance with highly correlated features. 

```{r fig.width=6.5, fig.height = 4.5}
set.seed(2021)
X <- MASS::mvrnorm(n = 500, mu = c(2, 2), 
                   Sigma = matrix(c(1, 0.9, 0.9, 1), nrow = 2))
colnames(X) <- c("X1", "X2")
plot(X)
points(rep(1.5, 500), X[, 2], col = "red")

```

## Feature importance and correlation

Partial depedence plots are often used to visualize the effects of the most important features in black-box models. The example uses a `randomForest` model from the `randomForest` package, which has readily available measures of feature importance. We select one of the primary features, 'pctKids2Par', for further exploration. 'pctKids2Par' presents highly correlation with several other predictiors, limiting the appropriatness of PDPs and ICE curves.  

```{r  fig.width=6.5, fig.height = 4.5}
## Get data
crimes <- read.csv(paste0("https://archive.ics.uci.edu/ml/machine-learning-",
                          "databases/00211/CommViolPredUnnormalizedData.txt"), 
                   header = F)

## Add column names
crimes_descr <- readLines(paste0("http://archive.ics.uci.edu/ml/datasets/",
                                 "Communities+and+Crime+Unnormalized"))
crimes_descr <- strsplit(crimes_descr[grepl(pattern = "<br>@attribute", 
                                            x = crimes_descr)], split = " ")
colnames(crimes) <- vapply(crimes_descr, FUN = function(rw) {rw[[2]]}, 
                           FUN.VALUE = "STRING")

## Replace "?" with NA
crimes[crimes == "?"] <- NA
crimes <- crimes[, colSums(is.na(crimes)) == 0]

## Convert all columns to numeric, add outcome
col_start <- which(names(crimes) == "pop")
end_start <- which(names(crimes) == "pctOfficDrugUnit")
data <- as.data.frame(sapply(crimes[, col_start:end_start], as.numeric))
outcome <- crimes$murdPerPop

## Train the model
set.seed(2021)
rf_mod <- randomForest(data, outcome, ntree = 5)


## Feature importance
varImpPlot(rf_mod, n.var = 10, main = "Feature Importance", pt.cex = 2)
cor_vals <- abs(cor(data[, "pctKids2Par"], 
                    data[, setdiff(colnames(data), "pctKids2Par")]))

## Correlation
hist(cor_vals, breaks = 30, xlab = "abs(correlation)", 
     main = "Correlation with pctKids2Par")
num <- quantile(cor_vals, 0.8)
abline(v = num, col = "red", lwd = 3)
text(x = num + 0.23, y = 10, " 80th Percentile")
```

## totalvis plots

```{r fig.width=6.5, fig.height = 4.5}
## totalvis plot
df <- plot(totalvis(rf_mod, data), num_load = 8, legend_cex = 1.0)
knitr::kable(df, row.names = F, align = "lc")

## pinplot
plot(totalvis(rf_mod, data, pin = "pctKids2Par"))

## pdp
plot(totalvis(rf_mod, data, feature = "pctKids2Par", pc_num = NULL))

## partial_effects (default)
diag_obj <- partial_effects(rf_mod, data, num_load = 8)
plot(diag_obj, legend_loc = "left", legend_cex = 0.74)

## partial_effects (non-differenced)
plot(diag_obj, differenced = FALSE, legend_loc = "topleft", legend_cex = 0.9)
```

## Additionally functionality

```{r fig.width=6.5, fig.height = 4.5}
## ice plot (default is centered)
vis_obj <- totalvis(rf_mod, data, ice = T, feature = "pctKids2Par")
plot(vis_obj, legend_cex = 1.0)

## ice plot (uncentered)
plot(vis_obj, legend_cex = 1.0, center = FALSE)

## Wrapper to scree plot
scree_plot(totalvis(rf_mod, data))
```